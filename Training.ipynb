{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Network\n",
    "\n",
    "In this notebook, we will train the CNN-RNN model for Image captioning\n",
    "\n",
    "CNN [ResNet](https://arxiv.org/pdf/1512.03385.pdf) model is used for feature extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pmund\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "from data_loader import get_loader\n",
    "from data_loader_val import get_loader as val_get_loader\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from nlp_utils import clean_sentence, bleu_score\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['annotations', 'images']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset dir path\n",
    "cocoapi_dir = r\"../cocoapi/\"\n",
    "\n",
    "import os\n",
    "folders = [folder for folder in os.listdir(\"../cocoapi/\")]\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Ensure the models folder exists\n",
    "os.makedirs(\"./models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128  # batch size\n",
    "vocab_threshold = 5  # minimum word count threshold\n",
    "vocab_from_file = True  # if True, load existing vocab file\n",
    "embed_size = 256  # dimensionality of image and word embeddings\n",
    "hidden_size = 512  # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 1  # number of training epochs\n",
    "save_every = 1  # determines frequency of saving model weights\n",
    "print_every = 20  # determines window for printing average loss\n",
    "log_file = \"training_log.txt\"  # name of file with saved training loss and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        # smaller edge of image resized to 256\n",
    "        transforms.Resize(256),\n",
    "        # get 224x224 crop from random location\n",
    "        transforms.RandomCrop(224),\n",
    "        # horizontally flip image with probability=0.5\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        # convert the PIL Image to a tensor\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "            (0.229, 0.224, 0.225),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\pmund\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=1.60s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|███████████████████████████████████████████████████████████▋            | 490112/591753 [01:10<00:13, 7724.26it/s]"
     ]
    }
   ],
   "source": [
    "# Build data loader.\n",
    "data_loader = get_loader(\n",
    "    transform=transform_train,\n",
    "    mode=\"train\",\n",
    "    batch_size=batch_size,\n",
    "    vocab_threshold=vocab_threshold,\n",
    "    vocab_from_file=vocab_from_file,\n",
    "    cocoapi_loc=cocoapi_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Encoder and RNN Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "# ----------- Encoder ------------\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        # disable learning for parameters\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "\n",
    "\n",
    "# --------- Decoder ----------\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_size: final embedding size of the CNN encoder\n",
    "            hidden_size: hidden size of the LSTM\n",
    "            vocab_size: size of the vocabulary\n",
    "            num_layers: number of layers of the LSTM\n",
    "        \"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        # Assigning hidden dimension\n",
    "        self.hidden_dim = hidden_size\n",
    "        # Map each word index to a dense word embedding tensor of embed_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        # Creating LSTM layer\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        # Initializing linear to apply at last of RNN layer for further prediction\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        # Initializing values for hidden and cell state\n",
    "        self.hidden = (torch.zeros(1, 1, hidden_size), torch.zeros(1, 1, hidden_size))\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: features tensor. shape is (bs, embed_size)\n",
    "            captions: captions tensor. shape is (bs, cap_length)\n",
    "        Returns:\n",
    "            outputs: scores of the linear layer\n",
    "\n",
    "        \"\"\"\n",
    "        # remove <end> token from captions and embed captions\n",
    "        cap_embedding = self.embed(\n",
    "            captions[:, :-1]\n",
    "        )  # (bs, cap_length) -> (bs, cap_length-1, embed_size)\n",
    "\n",
    "        embeddings = torch.cat((features.unsqueeze(dim=1), cap_embedding), dim=1)\n",
    "\n",
    "        #  getting output i.e. score and hidden layer.\n",
    "        # first value: all the hidden states throughout the sequence. second value: the most recent hidden state\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeddings\n",
    "        )  # (bs, cap_length, hidden_size), (1, bs, hidden_size)\n",
    "        outputs = self.linear(lstm_out)  # (bs, cap_length, vocab_size)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, inputs, states=None, max_len=20):\n",
    "        \"\"\"\n",
    "        accepts pre-processed image tensor (inputs) and returns predicted\n",
    "        sentence (list of tensor ids of length max_len)\n",
    "        Args:\n",
    "            inputs: shape is (1, 1, embed_size)\n",
    "            states: initial hidden state of the LSTM\n",
    "            max_len: maximum length of the predicted sentence\n",
    "\n",
    "        Returns:\n",
    "            res: list of predicted words indices\n",
    "        \"\"\"\n",
    "        res = []\n",
    "\n",
    "        # Now we feed the LSTM output and hidden states back into itself to get the caption\n",
    "        for i in range(max_len):\n",
    "            lstm_out, states = self.lstm(\n",
    "                inputs, states\n",
    "            )  # lstm_out: (1, 1, hidden_size)\n",
    "            outputs = self.linear(lstm_out.squeeze(dim=1))  # outputs: (1, vocab_size)\n",
    "            _, predicted_idx = outputs.max(dim=1)  # predicted: (1, 1)\n",
    "            res.append(predicted_idx.item())\n",
    "            # if the predicted idx is the stop index, the loop stops\n",
    "            if predicted_idx == 1:\n",
    "                break\n",
    "            inputs = self.embed(predicted_idx)  # inputs: (1, embed_size)\n",
    "            # prepare input for next iteration\n",
    "            inputs = inputs.unsqueeze(1)  # inputs: (1, 1, embed_size)\n",
    "\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size is :  11543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\capstone\\myvenv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\capstone\\myvenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "print(\"vocab size is : \",vocab_size)\n",
    "\n",
    "# Initializing the encoder and decoder\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Defining the loss function\n",
    "criterion = (\n",
    "    nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    ")\n",
    "\n",
    "# Specifying the learnable parameters of the mode\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Defining the optimize\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# Set the total number of training steps per epoc\n",
    "total_step = math.ceil(len(data_loader.dataset) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4624\n"
     ]
    }
   ],
   "source": [
    "print(total_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Capstone\\Image-Captioning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [20/4624], Loss: 5.1013, Perplexity: 164.2290\n",
      "Epoch [1/1], Step [40/4624], Loss: 4.3393, Perplexity: 76.6539\n",
      "Epoch [1/1], Step [60/4624], Loss: 4.4507, Perplexity: 85.6882\n",
      "Epoch [1/1], Step [80/4624], Loss: 3.8214, Perplexity: 45.6693\n",
      "Epoch [1/1], Step [100/4624], Loss: 3.7806, Perplexity: 43.8440\n",
      "Epoch [1/1], Step [120/4624], Loss: 3.8605, Perplexity: 47.4867\n",
      "Epoch [1/1], Step [140/4624], Loss: 3.8113, Perplexity: 45.2109\n",
      "Epoch [1/1], Step [160/4624], Loss: 3.5945, Perplexity: 36.3958\n",
      "Epoch [1/1], Step [180/4624], Loss: 3.5410, Perplexity: 34.5018\n",
      "Epoch [1/1], Step [200/4624], Loss: 3.4177, Perplexity: 30.4991\n",
      "Epoch [1/1], Step [220/4624], Loss: 3.7239, Perplexity: 41.4236\n",
      "Epoch [1/1], Step [240/4624], Loss: 3.2470, Perplexity: 25.7142\n",
      "Epoch [1/1], Step [260/4624], Loss: 3.2702, Perplexity: 26.3156\n",
      "Epoch [1/1], Step [280/4624], Loss: 3.2039, Perplexity: 24.6278\n",
      "Epoch [1/1], Step [300/4624], Loss: 3.3706, Perplexity: 29.0958\n",
      "Epoch [1/1], Step [320/4624], Loss: 3.3553, Perplexity: 28.6555\n",
      "Epoch [1/1], Step [340/4624], Loss: 3.2729, Perplexity: 26.3889\n",
      "Epoch [1/1], Step [360/4624], Loss: 3.7042, Perplexity: 40.6173\n",
      "Epoch [1/1], Step [380/4624], Loss: 3.3047, Perplexity: 27.2415\n",
      "Epoch [1/1], Step [400/4624], Loss: 3.2563, Perplexity: 25.9541\n",
      "Epoch [1/1], Step [420/4624], Loss: 3.3214, Perplexity: 27.6992\n",
      "Epoch [1/1], Step [440/4624], Loss: 3.1777, Perplexity: 23.9914\n",
      "Epoch [1/1], Step [460/4624], Loss: 3.2518, Perplexity: 25.8362\n",
      "Epoch [1/1], Step [480/4624], Loss: 3.0490, Perplexity: 21.0951\n",
      "Epoch [1/1], Step [500/4624], Loss: 3.1093, Perplexity: 22.4062\n",
      "Epoch [1/1], Step [520/4624], Loss: 3.4367, Perplexity: 31.0844\n",
      "Epoch [1/1], Step [540/4624], Loss: 2.9943, Perplexity: 19.9714\n",
      "Epoch [1/1], Step [560/4624], Loss: 3.3029, Perplexity: 27.1920\n",
      "Epoch [1/1], Step [580/4624], Loss: 3.5639, Perplexity: 35.3000\n",
      "Epoch [1/1], Step [600/4624], Loss: 3.0532, Perplexity: 21.1836\n",
      "Epoch [1/1], Step [620/4624], Loss: 2.8601, Perplexity: 17.4637\n",
      "Epoch [1/1], Step [640/4624], Loss: 3.0392, Perplexity: 20.8883\n",
      "Epoch [1/1], Step [660/4624], Loss: 2.8402, Perplexity: 17.1196\n",
      "Epoch [1/1], Step [680/4624], Loss: 3.2870, Perplexity: 26.7629\n",
      "Epoch [1/1], Step [700/4624], Loss: 2.8717, Perplexity: 17.6672\n",
      "Epoch [1/1], Step [720/4624], Loss: 2.9821, Perplexity: 19.7291\n",
      "Epoch [1/1], Step [740/4624], Loss: 2.9988, Perplexity: 20.0621\n",
      "Epoch [1/1], Step [760/4624], Loss: 2.8793, Perplexity: 17.8020\n",
      "Epoch [1/1], Step [780/4624], Loss: 3.2104, Perplexity: 24.7878\n",
      "Epoch [1/1], Step [800/4624], Loss: 2.9150, Perplexity: 18.4495\n",
      "Epoch [1/1], Step [820/4624], Loss: 2.7060, Perplexity: 14.9692\n",
      "Epoch [1/1], Step [840/4624], Loss: 2.8970, Perplexity: 18.1202\n",
      "Epoch [1/1], Step [860/4624], Loss: 2.8487, Perplexity: 17.2647\n",
      "Epoch [1/1], Step [880/4624], Loss: 3.0082, Perplexity: 20.2514\n",
      "Epoch [1/1], Step [900/4624], Loss: 2.8381, Perplexity: 17.0829\n",
      "Epoch [1/1], Step [920/4624], Loss: 2.7460, Perplexity: 15.5806\n",
      "Epoch [1/1], Step [940/4624], Loss: 2.5673, Perplexity: 13.0304\n",
      "Epoch [1/1], Step [960/4624], Loss: 2.7524, Perplexity: 15.6806\n",
      "Epoch [1/1], Step [980/4624], Loss: 2.7214, Perplexity: 15.2011\n",
      "Epoch [1/1], Step [1000/4624], Loss: 2.7096, Perplexity: 15.0238\n",
      "Epoch [1/1], Step [1020/4624], Loss: 2.9636, Perplexity: 19.3683\n",
      "Epoch [1/1], Step [1040/4624], Loss: 2.4633, Perplexity: 11.7431\n",
      "Epoch [1/1], Step [1060/4624], Loss: 2.6505, Perplexity: 14.1607\n",
      "Epoch [1/1], Step [1080/4624], Loss: 2.7414, Perplexity: 15.5080\n",
      "Epoch [1/1], Step [1100/4624], Loss: 2.9148, Perplexity: 18.4452\n",
      "Epoch [1/1], Step [1120/4624], Loss: 2.5661, Perplexity: 13.0150\n",
      "Epoch [1/1], Step [1140/4624], Loss: 3.1979, Perplexity: 24.4799\n",
      "Epoch [1/1], Step [1160/4624], Loss: 2.8366, Perplexity: 17.0582\n",
      "Epoch [1/1], Step [1180/4624], Loss: 2.6719, Perplexity: 14.4671\n",
      "Epoch [1/1], Step [1200/4624], Loss: 2.7331, Perplexity: 15.3808\n",
      "Epoch [1/1], Step [1220/4624], Loss: 2.5735, Perplexity: 13.1117\n",
      "Epoch [1/1], Step [1240/4624], Loss: 2.5581, Perplexity: 12.9114\n",
      "Epoch [1/1], Step [1260/4624], Loss: 2.5845, Perplexity: 13.2567\n",
      "Epoch [1/1], Step [1280/4624], Loss: 2.7434, Perplexity: 15.5402\n",
      "Epoch [1/1], Step [1300/4624], Loss: 2.3567, Perplexity: 10.5564\n",
      "Epoch [1/1], Step [1320/4624], Loss: 2.5189, Perplexity: 12.4150\n",
      "Epoch [1/1], Step [1340/4624], Loss: 2.4273, Perplexity: 11.3277\n",
      "Epoch [1/1], Step [1360/4624], Loss: 2.5278, Perplexity: 12.5264\n",
      "Epoch [1/1], Step [1380/4624], Loss: 2.5298, Perplexity: 12.5513\n",
      "Epoch [1/1], Step [1400/4624], Loss: 2.6943, Perplexity: 14.7957\n",
      "Epoch [1/1], Step [1420/4624], Loss: 2.6124, Perplexity: 13.6323\n",
      "Epoch [1/1], Step [1440/4624], Loss: 2.8467, Perplexity: 17.2302\n",
      "Epoch [1/1], Step [1460/4624], Loss: 2.3372, Perplexity: 10.3522\n",
      "Epoch [1/1], Step [1480/4624], Loss: 2.6276, Perplexity: 13.8400\n",
      "Epoch [1/1], Step [1500/4624], Loss: 2.6533, Perplexity: 14.2014\n",
      "Epoch [1/1], Step [1520/4624], Loss: 2.5912, Perplexity: 13.3460\n",
      "Epoch [1/1], Step [1540/4624], Loss: 2.5482, Perplexity: 12.7847\n",
      "Epoch [1/1], Step [1560/4624], Loss: 2.4125, Perplexity: 11.1616\n",
      "Epoch [1/1], Step [1580/4624], Loss: 2.5191, Perplexity: 12.4175\n",
      "Epoch [1/1], Step [1600/4624], Loss: 2.4657, Perplexity: 11.7712\n",
      "Epoch [1/1], Step [1620/4624], Loss: 2.4016, Perplexity: 11.0407\n",
      "Epoch [1/1], Step [1640/4624], Loss: 2.3913, Perplexity: 10.9282\n",
      "Epoch [1/1], Step [1660/4624], Loss: 2.8216, Perplexity: 16.8043\n",
      "Epoch [1/1], Step [1680/4624], Loss: 2.3575, Perplexity: 10.5645\n",
      "Epoch [1/1], Step [1700/4624], Loss: 2.6998, Perplexity: 14.8762\n",
      "Epoch [1/1], Step [1720/4624], Loss: 2.6423, Perplexity: 14.0453\n",
      "Epoch [1/1], Step [1740/4624], Loss: 2.4124, Perplexity: 11.1609\n",
      "Epoch [1/1], Step [1760/4624], Loss: 2.4747, Perplexity: 11.8781\n",
      "Epoch [1/1], Step [1780/4624], Loss: 2.5097, Perplexity: 12.3015\n",
      "Epoch [1/1], Step [1800/4624], Loss: 2.4453, Perplexity: 11.5343\n",
      "Epoch [1/1], Step [1820/4624], Loss: 2.4257, Perplexity: 11.3101\n",
      "Epoch [1/1], Step [1840/4624], Loss: 2.3517, Perplexity: 10.5039\n",
      "Epoch [1/1], Step [1860/4624], Loss: 2.4592, Perplexity: 11.6953\n",
      "Epoch [1/1], Step [1880/4624], Loss: 2.2586, Perplexity: 9.5701\n",
      "Epoch [1/1], Step [1900/4624], Loss: 2.4141, Perplexity: 11.1793\n",
      "Epoch [1/1], Step [1920/4624], Loss: 2.4990, Perplexity: 12.1708\n",
      "Epoch [1/1], Step [1940/4624], Loss: 2.4313, Perplexity: 11.3733\n",
      "Epoch [1/1], Step [1960/4624], Loss: 2.3733, Perplexity: 10.7324\n",
      "Epoch [1/1], Step [1980/4624], Loss: 3.6728, Perplexity: 39.3639\n",
      "Epoch [1/1], Step [2000/4624], Loss: 2.3868, Perplexity: 10.8783\n",
      "Epoch [1/1], Step [2020/4624], Loss: 2.4497, Perplexity: 11.5847\n",
      "Epoch [1/1], Step [2040/4624], Loss: 2.2421, Perplexity: 9.4131\n",
      "Epoch [1/1], Step [2060/4624], Loss: 2.2647, Perplexity: 9.6280\n",
      "Epoch [1/1], Step [2080/4624], Loss: 2.3989, Perplexity: 11.0114\n",
      "Epoch [1/1], Step [2100/4624], Loss: 2.2591, Perplexity: 9.5748\n",
      "Epoch [1/1], Step [2120/4624], Loss: 2.3186, Perplexity: 10.1612\n",
      "Epoch [1/1], Step [2140/4624], Loss: 2.2717, Perplexity: 9.6960\n",
      "Epoch [1/1], Step [2160/4624], Loss: 2.3139, Perplexity: 10.1140\n",
      "Epoch [1/1], Step [2180/4624], Loss: 2.3009, Perplexity: 9.9833\n",
      "Epoch [1/1], Step [2200/4624], Loss: 2.3868, Perplexity: 10.8789\n",
      "Epoch [1/1], Step [2220/4624], Loss: 2.3048, Perplexity: 10.0219\n",
      "Epoch [1/1], Step [2240/4624], Loss: 2.3294, Perplexity: 10.2718\n",
      "Epoch [1/1], Step [2260/4624], Loss: 2.2492, Perplexity: 9.4805\n",
      "Epoch [1/1], Step [2280/4624], Loss: 2.7750, Perplexity: 16.0379\n",
      "Epoch [1/1], Step [2300/4624], Loss: 2.2580, Perplexity: 9.5637\n",
      "Epoch [1/1], Step [2320/4624], Loss: 2.3400, Perplexity: 10.3808\n",
      "Epoch [1/1], Step [2340/4624], Loss: 2.3980, Perplexity: 11.0010\n",
      "Epoch [1/1], Step [2360/4624], Loss: 2.3165, Perplexity: 10.1402\n",
      "Epoch [1/1], Step [2380/4624], Loss: 2.3230, Perplexity: 10.2062\n",
      "Epoch [1/1], Step [2400/4624], Loss: 2.2675, Perplexity: 9.6556\n",
      "Epoch [1/1], Step [2420/4624], Loss: 2.3996, Perplexity: 11.0184\n",
      "Epoch [1/1], Step [2440/4624], Loss: 3.4974, Perplexity: 33.0296\n",
      "Epoch [1/1], Step [2460/4624], Loss: 2.4834, Perplexity: 11.9821\n",
      "Epoch [1/1], Step [2480/4624], Loss: 2.3231, Perplexity: 10.2076\n",
      "Epoch [1/1], Step [2500/4624], Loss: 2.3032, Perplexity: 10.0064\n",
      "Epoch [1/1], Step [2520/4624], Loss: 2.2576, Perplexity: 9.5605\n",
      "Epoch [1/1], Step [2540/4624], Loss: 2.3031, Perplexity: 10.0054\n",
      "Epoch [1/1], Step [2560/4624], Loss: 2.4316, Perplexity: 11.3776\n",
      "Epoch [1/1], Step [2580/4624], Loss: 2.4254, Perplexity: 11.3069\n",
      "Epoch [1/1], Step [2600/4624], Loss: 2.1438, Perplexity: 8.5318\n",
      "Epoch [1/1], Step [2620/4624], Loss: 2.4056, Perplexity: 11.0856\n",
      "Epoch [1/1], Step [2640/4624], Loss: 2.5797, Perplexity: 13.1932\n",
      "Epoch [1/1], Step [2660/4624], Loss: 2.4753, Perplexity: 11.8858\n",
      "Epoch [1/1], Step [2680/4624], Loss: 2.2007, Perplexity: 9.0315\n",
      "Epoch [1/1], Step [2700/4624], Loss: 2.3878, Perplexity: 10.8891\n",
      "Epoch [1/1], Step [2720/4624], Loss: 2.2648, Perplexity: 9.6295\n",
      "Epoch [1/1], Step [2740/4624], Loss: 2.4538, Perplexity: 11.6324\n",
      "Epoch [1/1], Step [2760/4624], Loss: 2.0176, Perplexity: 7.5202\n",
      "Epoch [1/1], Step [2780/4624], Loss: 2.2015, Perplexity: 9.0389\n",
      "Epoch [1/1], Step [2800/4624], Loss: 2.1774, Perplexity: 8.8233\n",
      "Epoch [1/1], Step [2820/4624], Loss: 2.2369, Perplexity: 9.3647\n",
      "Epoch [1/1], Step [2840/4624], Loss: 2.5871, Perplexity: 13.2907\n",
      "Epoch [1/1], Step [2860/4624], Loss: 3.0240, Perplexity: 20.5740\n",
      "Epoch [1/1], Step [2880/4624], Loss: 2.8507, Perplexity: 17.2995\n",
      "Epoch [1/1], Step [2900/4624], Loss: 2.2403, Perplexity: 9.3963\n",
      "Epoch [1/1], Step [2920/4624], Loss: 2.1914, Perplexity: 8.9477\n",
      "Epoch [1/1], Step [2940/4624], Loss: 2.2584, Perplexity: 9.5674\n",
      "Epoch [1/1], Step [2960/4624], Loss: 2.2140, Perplexity: 9.1519\n",
      "Epoch [1/1], Step [2980/4624], Loss: 2.1925, Perplexity: 8.9577\n",
      "Epoch [1/1], Step [3000/4624], Loss: 2.1354, Perplexity: 8.4605\n",
      "Epoch [1/1], Step [3020/4624], Loss: 2.1793, Perplexity: 8.8399\n",
      "Epoch [1/1], Step [3040/4624], Loss: 2.1681, Perplexity: 8.7413\n",
      "Epoch [1/1], Step [3060/4624], Loss: 2.3888, Perplexity: 10.9004\n",
      "Epoch [1/1], Step [3080/4624], Loss: 3.2061, Perplexity: 24.6821\n",
      "Epoch [1/1], Step [3100/4624], Loss: 2.1926, Perplexity: 8.9582\n",
      "Epoch [1/1], Step [3120/4624], Loss: 2.4348, Perplexity: 11.4134\n",
      "Epoch [1/1], Step [3140/4624], Loss: 2.2457, Perplexity: 9.4472\n",
      "Epoch [1/1], Step [3160/4624], Loss: 2.0948, Perplexity: 8.1238\n",
      "Epoch [1/1], Step [3180/4624], Loss: 2.3557, Perplexity: 10.5454\n",
      "Epoch [1/1], Step [3200/4624], Loss: 2.2788, Perplexity: 9.7649\n",
      "Epoch [1/1], Step [3220/4624], Loss: 2.2751, Perplexity: 9.7289\n",
      "Epoch [1/1], Step [3240/4624], Loss: 2.2423, Perplexity: 9.4149\n",
      "Epoch [1/1], Step [3260/4624], Loss: 2.3102, Perplexity: 10.0767\n",
      "Epoch [1/1], Step [3280/4624], Loss: 2.2888, Perplexity: 9.8631\n",
      "Epoch [1/1], Step [3300/4624], Loss: 2.0836, Perplexity: 8.0334\n",
      "Epoch [1/1], Step [3320/4624], Loss: 2.1131, Perplexity: 8.2740\n",
      "Epoch [1/1], Step [3340/4624], Loss: 2.4366, Perplexity: 11.4342\n",
      "Epoch [1/1], Step [3360/4624], Loss: 2.2873, Perplexity: 9.8484\n",
      "Epoch [1/1], Step [3380/4624], Loss: 2.3251, Perplexity: 10.2281\n",
      "Epoch [1/1], Step [3400/4624], Loss: 2.1293, Perplexity: 8.4089\n",
      "Epoch [1/1], Step [3420/4624], Loss: 2.1784, Perplexity: 8.8325\n",
      "Epoch [1/1], Step [3440/4624], Loss: 2.0777, Perplexity: 7.9859\n",
      "Epoch [1/1], Step [3460/4624], Loss: 2.1336, Perplexity: 8.4449\n",
      "Epoch [1/1], Step [3480/4624], Loss: 2.1994, Perplexity: 9.0199\n",
      "Epoch [1/1], Step [3500/4624], Loss: 2.0548, Perplexity: 7.8056\n",
      "Epoch [1/1], Step [3520/4624], Loss: 2.1152, Perplexity: 8.2909\n",
      "Epoch [1/1], Step [3540/4624], Loss: 2.4650, Perplexity: 11.7640\n",
      "Epoch [1/1], Step [3560/4624], Loss: 2.2254, Perplexity: 9.2568\n",
      "Epoch [1/1], Step [3580/4624], Loss: 2.0965, Perplexity: 8.1374\n",
      "Epoch [1/1], Step [3600/4624], Loss: 2.2995, Perplexity: 9.9695\n",
      "Epoch [1/1], Step [3620/4624], Loss: 2.1755, Perplexity: 8.8066\n",
      "Epoch [1/1], Step [3640/4624], Loss: 2.0637, Perplexity: 7.8752\n",
      "Epoch [1/1], Step [3660/4624], Loss: 2.2309, Perplexity: 9.3079\n",
      "Epoch [1/1], Step [3680/4624], Loss: 2.1412, Perplexity: 8.5097\n",
      "Epoch [1/1], Step [3700/4624], Loss: 2.1128, Perplexity: 8.2718\n",
      "Epoch [1/1], Step [3720/4624], Loss: 2.1706, Perplexity: 8.7638\n",
      "Epoch [1/1], Step [3740/4624], Loss: 2.1883, Perplexity: 8.9203\n",
      "Epoch [1/1], Step [3760/4624], Loss: 2.2515, Perplexity: 9.5022\n",
      "Epoch [1/1], Step [3780/4624], Loss: 2.0588, Perplexity: 7.8363\n",
      "Epoch [1/1], Step [3800/4624], Loss: 2.5109, Perplexity: 12.3155\n",
      "Epoch [1/1], Step [3820/4624], Loss: 2.3041, Perplexity: 10.0152\n",
      "Epoch [1/1], Step [3840/4624], Loss: 2.0991, Perplexity: 8.1591\n",
      "Epoch [1/1], Step [3860/4624], Loss: 2.2911, Perplexity: 9.8858\n",
      "Epoch [1/1], Step [3880/4624], Loss: 2.1162, Perplexity: 8.2993\n",
      "Epoch [1/1], Step [3900/4624], Loss: 2.2679, Perplexity: 9.6592\n",
      "Epoch [1/1], Step [3920/4624], Loss: 2.2402, Perplexity: 9.3955\n",
      "Epoch [1/1], Step [3940/4624], Loss: 2.0433, Perplexity: 7.7163\n",
      "Epoch [1/1], Step [3960/4624], Loss: 2.0363, Perplexity: 7.6623\n",
      "Epoch [1/1], Step [3980/4624], Loss: 2.1689, Perplexity: 8.7483\n",
      "Epoch [1/1], Step [4000/4624], Loss: 2.8333, Perplexity: 17.0023\n",
      "Epoch [1/1], Step [4020/4624], Loss: 2.0280, Perplexity: 7.5987\n",
      "Epoch [1/1], Step [4040/4624], Loss: 2.2238, Perplexity: 9.2425\n",
      "Epoch [1/1], Step [4060/4624], Loss: 1.9601, Perplexity: 7.1003\n",
      "Epoch [1/1], Step [4080/4624], Loss: 2.0834, Perplexity: 8.0314\n",
      "Epoch [1/1], Step [4100/4624], Loss: 2.3387, Perplexity: 10.3673\n",
      "Epoch [1/1], Step [4120/4624], Loss: 2.5366, Perplexity: 12.6363\n",
      "Epoch [1/1], Step [4140/4624], Loss: 2.1457, Perplexity: 8.5478\n",
      "Epoch [1/1], Step [4160/4624], Loss: 2.0196, Perplexity: 7.5352\n",
      "Epoch [1/1], Step [4180/4624], Loss: 2.0311, Perplexity: 7.6221\n",
      "Epoch [1/1], Step [4200/4624], Loss: 2.4459, Perplexity: 11.5409\n",
      "Epoch [1/1], Step [4220/4624], Loss: 2.0787, Perplexity: 7.9941\n",
      "Epoch [1/1], Step [4240/4624], Loss: 2.3588, Perplexity: 10.5781\n",
      "Epoch [1/1], Step [4260/4624], Loss: 1.9449, Perplexity: 6.9931\n",
      "Epoch [1/1], Step [4280/4624], Loss: 1.9794, Perplexity: 7.2381\n",
      "Epoch [1/1], Step [4300/4624], Loss: 2.1641, Perplexity: 8.7066\n",
      "Epoch [1/1], Step [4320/4624], Loss: 2.1389, Perplexity: 8.4903\n",
      "Epoch [1/1], Step [4340/4624], Loss: 2.1470, Perplexity: 8.5591\n",
      "Epoch [1/1], Step [4360/4624], Loss: 2.5220, Perplexity: 12.4541\n",
      "Epoch [1/1], Step [4380/4624], Loss: 2.0743, Perplexity: 7.9588\n",
      "Epoch [1/1], Step [4400/4624], Loss: 2.1812, Perplexity: 8.8566\n",
      "Epoch [1/1], Step [4420/4624], Loss: 2.1534, Perplexity: 8.6139\n",
      "Epoch [1/1], Step [4440/4624], Loss: 2.0781, Perplexity: 7.9893\n",
      "Epoch [1/1], Step [4460/4624], Loss: 3.1067, Perplexity: 22.3465\n",
      "Epoch [1/1], Step [4480/4624], Loss: 2.4234, Perplexity: 11.2846\n",
      "Epoch [1/1], Step [4500/4624], Loss: 2.0117, Perplexity: 7.4764\n",
      "Epoch [1/1], Step [4520/4624], Loss: 2.0773, Perplexity: 7.9831\n",
      "Epoch [1/1], Step [4540/4624], Loss: 2.1141, Perplexity: 8.2817\n",
      "Epoch [1/1], Step [4560/4624], Loss: 2.2529, Perplexity: 9.5156\n",
      "Epoch [1/1], Step [4580/4624], Loss: 2.1316, Perplexity: 8.4283\n",
      "Epoch [1/1], Step [4600/4624], Loss: 2.0793, Perplexity: 7.9986\n",
      "Epoch [1/1], Step [4620/4624], Loss: 2.2757, Perplexity: 9.7350\n"
     ]
    }
   ],
   "source": [
    "# Open the training log file.\n",
    "f = open(log_file, \"w\")\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for i_step in range(1, total_step + 1):\n",
    "\n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "\n",
    "        # Passing the inputs through the CNN-RNN model\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "\n",
    "        # Calculating the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "\n",
    "        # Backwarding pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating the parameters in the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # Getting training statistics\n",
    "        stats = (\n",
    "            f\"Epoch [{epoch}/{num_epochs}], Step [{i_step}/{total_step}], \"\n",
    "            f\"Loss: {loss.item():.4f}, Perplexity: {np.exp(loss.item()):.4f}\"\n",
    "        )\n",
    "\n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + \"\\n\")\n",
    "        f.flush()\n",
    "\n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print(\"\\r\" + stats)\n",
    "\n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(\n",
    "            decoder.state_dict(), os.path.join(\"./models\", \"decoder-%d.pkl\" % epoch)\n",
    "        )\n",
    "        torch.save(\n",
    "            encoder.state_dict(), os.path.join(\"./models\", \"encoder-%d.pkl\" % epoch)\n",
    "        )\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Validating the Model using Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (embed): Embedding(11543, 256)\n",
       "  (lstm): LSTM(256, 512, batch_first=True)\n",
       "  (linear): Linear(in_features=512, out_features=11543, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "            (0.229, 0.224, 0.225),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#Create the data loader.\n",
    "val_data_loader = val_get_loader(\n",
    "    transform=transform_test, mode=\"valid\", cocoapi_loc=cocoapi_dir\n",
    ")\n",
    "\n",
    "\n",
    "encoder_file = \"encoder-1.pkl\"\n",
    "decoder_file = \"decoder-1.pkl\"\n",
    "\n",
    "# Initialize the encoder and decoder.\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Moving models to GPU if CUDA is available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Loading the trained weights\n",
    "encoder.load_state_dict(torch.load(os.path.join(\"./models\", encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join(\"./models\", decoder_file)))\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490898d26cf244afa1327c311be16be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# infer captions for all images\n",
    "pred_result = defaultdict(list)\n",
    "for img_id, img in tqdm(val_data_loader):\n",
    "    img = img.to(device)\n",
    "    with torch.no_grad():\n",
    "        features = encoder(img).unsqueeze(1)\n",
    "        output = decoder.sample(features)\n",
    "    sentence = clean_sentence(output, val_data_loader.dataset.vocab.idx2word)\n",
    "    pred_result[img_id.item()].append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    #os.path.join(cocoapi_dir, \"cocoapi\", \"annotations/captions_val2014.json\"), \"r\"\n",
    "    os.path.join(cocoapi_dir, \"annotations/captions_val2017.json\"), \"r\"\n",
    ") as f:\n",
    "    caption = json.load(f)\n",
    "\n",
    "valid_annot = caption[\"annotations\"]\n",
    "valid_result = defaultdict(list)\n",
    "for i in valid_annot:\n",
    "    valid_result[i[\"image_id\"]].append(i[\"caption\"].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a black honda motorcycle parked in front of a garage.',\n",
       "  'a honda motorcycle parked in a grass driveway',\n",
       "  'a black honda motorcycle with a dark burgundy seat.',\n",
       "  'ma motorcycle parked on the gravel in front of a garage',\n",
       "  'a motorcycle with its brake extended standing outside'],\n",
       " ['an office cubicle with four different types of computers.',\n",
       "  'the home office space seems to be very cluttered.',\n",
       "  'an office with desk computer and chair and laptop.',\n",
       "  'office setting with a lot of computer screens.',\n",
       "  'a desk and chair in an office cubicle.'],\n",
       " ['a small closed toilet in a cramped space.',\n",
       "  'a tan toilet and sink combination in a small room.',\n",
       "  'this is an advanced toilet with a sink and control panel.',\n",
       "  'a close-up picture of a toilet with a fountain.',\n",
       "  'off white toilet with a faucet and controls. ']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(valid_result.values())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' a bird is standing on the beach with a bird .'],\n",
       " [' a bunch of bananas hanging from a tree .'],\n",
       " [' a large airplane flying through the air .']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pred_result.values())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1680677280835314"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_score(true_sentences=valid_result, predicted_sentences=pred_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a bad bleu score with only 3 epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['decoder-1.pkl', 'encoder-1.pkl']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"./models\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
